{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe995c4c-2c53-494f-b6e7-f94f98039e68",
   "metadata": {},
   "source": [
    "---\n",
    "## Multiple Linear Regression (Not MultiVariate Regression)\n",
    "\n",
    "Now, we will make Linear Regression much more faster and powerful.\n",
    "\n",
    "Let's start by looking at the version of linear regression that look at not just one feature, but a lot of different features.\n",
    "\n",
    "### Multiple Features (or Variables)\n",
    "- #### Now, we will introduce $x_j$ as the $j^{th}$ feature or variable in our list of features or variables.\n",
    "- #### $n$ - Total no. of features or variables.\n",
    "- #### $\\vec{x}^{(i)}$ - All $x_j$ ( j from 1 to n) features of $i^{(th)}$ training example.\n",
    "    - #### $\\vec{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}, ~...~ x_n^{(i)}]$ - This is usually called a **Row Vector** rather than a **Column Vector**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b933a6a5-a0aa-4ef6-8e0b-ca4db0210522",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Model\n",
    "- #### Previously - $f_{(w, b)}(x) = wx + b$\n",
    "- #### Now - $$f_{(\\vec{w}, b)}(\\vec{x}) = w_1x_1 + w_2x_2 + w_3x_3 ~+~ ... ~+~ w_nx_n + b$$\n",
    "    - #### $b~$ It is called the `Base Parameter`, i.e. when all other input features are zero. This will be our Model's value. It is not a Vector.\n",
    "    - #### $\\vec{w} = [w_1,~ w_2,~ w_3,~~ ...~ w_n]~$ This is a **Vector** or more specifically a **Row Vector**.\n",
    "    - #### Above two are the **Parameters of the Model**.\n",
    "- #### New Model can be Rewritten as - $$f_{(\\vec{w}, b)}(\\vec{x}) = \\vec{w} \\cdot \\vec{x} + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e288c3f-6547-473f-9b50-4a7560493d99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Vectorization\n",
    "\n",
    "##### It helps in implementing Multiple Linear Regression in Machine Learning. \n",
    "##### When you're implementing a learning algorithm, using vectorization will both make your code shorter and also make it run much more efficiently.\n",
    "\n",
    "##### Here's an example with parameters w and b, where;\n",
    "    \n",
    "- $\\vec{w} = [w_1, w_2, w_3]$\n",
    "- $\\vec{x} = [x_1, x_2, x_3]$\n",
    "- $b$ is a number\n",
    "- $n = 3$\n",
    "\n",
    "#### Defining, above parameters in python;\n",
    "```python\n",
    "w = np.array([1.0, 25, -3.3])\n",
    "b = 4\n",
    "x = np.array([10, 20, 30])\n",
    "```\n",
    "\n",
    "#### Without Vectorization\n",
    "$$f_{(\\vec{w}, b)}(\\vec{x}) = w_1x_1 + w_2x_2 + w_3x_3 + b$$\n",
    "- ##### Without vectorization, above equation in python would look like as;\n",
    "```python\n",
    "f = w[0] * x[0] + \n",
    "    w[1] * x[1] + \n",
    "    w[2] * x[2] + b\n",
    "```\n",
    "- ##### This will cause issues when $n$ is large.\n",
    "\n",
    "#### With Vectorization\n",
    "- ##### Above equation can be written using vectors as given below.\n",
    "$$\n",
    "f_{\\vec{w}, b}(\\vec{x}) = \\vec{w} \\cdot \\vec{x} + b\n",
    "$$\n",
    "\n",
    "- Defining, above in python;\n",
    "    - ```python\n",
    "      f = np.dot(w, x) + b\n",
    "      ```\n",
    "- ##### This would run much faster than other previous examples. This is also practically possible when $n$ is large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200bd2ea-bd18-41b0-819c-3064fb9feac1",
   "metadata": {},
   "source": [
    "---\n",
    "### Let's implement Multiple Linear Regression with Vectorization\n",
    "- #### Parameters\n",
    "    - #### $\\vec{w} = (w_1, w_2,~ ...~w_n)~$ Vector\n",
    "    - #### $b~$ Scalar\n",
    " - #### Model\n",
    "     - #### $f_{(\\vec{w}, b)}(\\vec{x}) = \\vec{w} \\cdot \\vec{x} + b$\n",
    " - #### Cost Function\n",
    "     - #### $J(\\vec{w}, b)$\n",
    " - #### Gradient Descent\n",
    "     - #### repeat { <br><div style=\"padding-left: 20px;\"> $w_j = w_j - \\alpha \\frac{\\partial J(\\vec{w}, b)}{\\partial w_j}$ </div> <br><div style=\"padding-left: 20px;\"> $b = b - \\alpha \\frac{\\partial J(\\vec{w}, b)}{\\partial b}$ </div> <br>}\n",
    "\n",
    "<h4>\n",
    "Gradient Descent becomes just a little bit different with multiple features compared to just one feature.\n",
    "</h4>\n",
    "\n",
    "|                     <h2>One Feature</h2>                   |           <h2>$n~$ Features ($~n \\geq 2~$)</h2>            |\n",
    "| :----------------------------------------------------------: | ---------------------------------------------------------- |\n",
    "| <img width=\"70%\" src=\"http://localhost:8888/files/Supervised%20Models/Regression/Assets/gradient_descent_single_LR.png?_xsrf=2%7C9a187810%7Cce664c5d0ea99f104da27a491ca0227f%7C1720535697\"> | <img width=\"70%\" src=\"http://localhost:8888/files/Supervised%20Models/Regression/Assets/gradient_descent_multiple_LR.png?_xsrf=2%7C9a187810%7Cce664c5d0ea99f104da27a491ca0227f%7C1720535697\"> |\n",
    "\n",
    "<h4>\n",
    "    That's it for gradient descent for multiple regression.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37249a2f-70b5-4edf-8f63-f029b2e322db",
   "metadata": {},
   "source": [
    "---\n",
    "## An Alternative to Gradient Descent\n",
    "##### There is an alternative way for finding w and b for linear regression. This method is called the normal equation. Whereas it turns out gradient descent is a great method for minimizing the cost function J to find w and b, there is one other algorithm that works only for linear regression and pretty much none of the other algorithms you see in Machine Learning for solving for w and b and this other method does not need an iterative gradient descent algorithm.\n",
    "\n",
    "#### Normal Equation\n",
    "- ##### Only for Linear Regression\n",
    "- ##### Solve for $w,~ b~$ without iterations\n",
    "- ##### Disadvantages\n",
    "    - ##### Doesn't generalize to other learning algorithms such as logistic regression algorithm or the neural networks or others.\n",
    "    - ##### Slow when $n~$ i.e. no. of features is large (> 10000)\n",
    "- ##### Normal equation methods may be used in machine learning libraries that implement linear regression\n",
    "- #### Gradient Descent is the recommended method for finding parameters $w$ and $b$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92e9fb68-3f61-4fbe-8ee2-d7ac730b4a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/friday_code/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
